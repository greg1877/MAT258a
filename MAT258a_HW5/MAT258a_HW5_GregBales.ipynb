{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "H_1_hes (generic function with 1 method)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using ForwardDiff\n",
    "\n",
    "#Function Definition for Hock-Schittkowski Collection, problems 6 through 9\n",
    "\n",
    "#Problem 6 ########################################\n",
    "function F_6(x)\n",
    "    return (1-x[1])^2;\n",
    "end;\n",
    "#F_6_x0 = [-1.2,1.0];\n",
    "F_6_x0 = [0.0,0.0];\n",
    "\n",
    "function H_6(x)\n",
    "    return 10*(x[2]-x[1]^2);\n",
    "end;\n",
    "\n",
    "F_6_grd(x) = ForwardDiff.gradient(x -> F_6(x))(x);\n",
    "F_6_hes(x) = ForwardDiff.hessian(x -> F_6(x))(x);\n",
    "\n",
    "H_6_grd(x) = ForwardDiff.gradient(x -> H_6(x))(x);\n",
    "H_6_hes(x) = ForwardDiff.hessian(x -> H_6(x))(x);\n",
    "###################################################\n",
    "\n",
    "#Problem 7 ########################################\n",
    "function F_7(x)\n",
    "    return log(1.0+x[1]^2)-x[2];\n",
    "end;\n",
    "#F_7_x0 = [2.0,2.0];\n",
    "F_7_x0 = [1.0,sqrt(3)]; #a feasible point\n",
    "\n",
    "function H_7(x)\n",
    "    return (1+x[1]^2)^2 + x[2]^2 - 4.0;\n",
    "end;\n",
    "\n",
    "F_7_grd(x) = ForwardDiff.gradient(x -> F_7(x))(x);\n",
    "F_7_hes(x) = ForwardDiff.hessian(x -> F_7(x))(x);\n",
    "\n",
    "H_7_grd(x) = ForwardDiff.gradient(x -> H_7(x))(x);\n",
    "H_7_hes(x) = ForwardDiff.hessian(x -> H_7(x))(x);\n",
    "###################################################\n",
    "\n",
    "#Problem 8 ########################################\n",
    "function F_8(x)\n",
    "    return -1;\n",
    "end;\n",
    "#F_8_x0 = [2.0,1.0];\n",
    "F_8_x0 =[0.0,0.0];\n",
    "\n",
    "function H_8_1(x) #first constraint\n",
    "    return x[1]^2 + x[2]^2 - 25.0;\n",
    "end;\n",
    "\n",
    "function H_8_2(x) #second constraint\n",
    "    return x[1]*x[2] - 9.0;\n",
    "end;\n",
    "\n",
    "F_8_grd(x) = ForwardDiff.gradient(x -> F_8(x))(x);\n",
    "F_8_hes(x) = ForwardDiff.hessian(x -> F_8(x))(x);\n",
    "\n",
    "H_8_1_grd(x) = ForwardDiff.gradient(x -> H_8_1(x))(x);\n",
    "H_8_1_hes(x) = ForwardDiff.hessian(x -> H_8_1(x))(x);\n",
    "\n",
    "H_8_2_grd(x) = ForwardDiff.gradient(x -> H_8_2(x))(x);\n",
    "H_8_2_hes(x) = ForwardDiff.hessian(x -> H_8_2(x))(x);\n",
    "###################################################\n",
    "\n",
    "#Problem 9 ########################################\n",
    "function F_9(x)\n",
    "    return sin(pi*x[1]/12.0)*cos(pi*x[2]/12.0);\n",
    "end;\n",
    "F_9_x0 = [0.0,0.0]; #a feasible point\n",
    "\n",
    "function H_9(x)\n",
    "    return 4*x[1] - 3*x[2];\n",
    "end;\n",
    "\n",
    "F_9_grd(x) = ForwardDiff.gradient(x -> F_9(x))(x);\n",
    "F_9_hes(x) = ForwardDiff.hessian(x -> F_9(x))(x);\n",
    "\n",
    "H_9_grd(x) = ForwardDiff.gradient(x -> H_9(x))(x);\n",
    "H_9_hes(x) = ForwardDiff.hessian(x -> H_9(x))(x);\n",
    "###################################################\n",
    "\n",
    "#Test Problem #####################################\n",
    "function F_1(x)\n",
    "    return x[1]+x[2];\n",
    "end;\n",
    "F_1_x0 = [0.3,0.3]; #a feasible point\n",
    "\n",
    "function H_1(x)\n",
    "    return x[1]^2 + x[2]^2 - 2;\n",
    "end;\n",
    "\n",
    "F_1_grd(x) = ForwardDiff.gradient(x -> F_1(x))(x);\n",
    "F_1_hes(x) = ForwardDiff.hessian(x -> F_1(x))(x);\n",
    "\n",
    "H_1_grd(x) = ForwardDiff.gradient(x -> H_1(x))(x);\n",
    "H_1_hes(x) = ForwardDiff.hessian(x -> H_1(x))(x);\n",
    "###################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lagrangian (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function lagrangian(f, h, x_k0, lambda, rho)\n",
    "    #take in the function definitions and spit \n",
    "    #out the lagrangian for given constraints, lambda and rho\n",
    "    (f_k0, fgrad_k0, fhes_k0) = f(x_k0);\n",
    "    (h_k0, hgrad_k0, hhes_k0) = h(x_k0);\n",
    "    \n",
    "    L_k0 = f_k0 - lambda*h_k0 + (rho/2)*(norm(h_k0))^2;\n",
    "    Lgrd_k0 = fgrad_k0 + (-lambda + rho*h_k0)*hgrad_k0;\n",
    "    Lhes_k0 = eye(2);\n",
    "    return (L_k0, Lgrd_k0, Lhes_k0);\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "newtmin_BFGS_HW5 (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function newtmin_BFGS_HW5(f, h, x_k0, lambda, rho, maxIts, optTol)  \n",
    "    # obj is a function that evaluates the objective value, \n",
    "    # gradient and hessian a x.\n",
    "    # x0 is the starting point.\n",
    "    # maxIts = maximum interations\n",
    "    # optTol is the optimality tolerance on the gradient\n",
    "    \n",
    "    its = 1;\n",
    "    Tol = 0;\n",
    "    err = 0;\n",
    "    fault_count = 0;\n",
    "    \n",
    "    delta = 1;\n",
    "    \n",
    "    # calculate the gradient, hessian and objective at x \n",
    "    #(f_k0, g_k0, h_k0) = f(x_k0);\n",
    "    (f_k0, g_k0, h_k0) = lagrangian(f, h, x_k0, lambda, rho);\n",
    "    \n",
    "    tolFactor = optTol*norm(g_k0);\n",
    "\n",
    "    B_k0 = h_k0;\n",
    "    \n",
    "    # Start the Loop \n",
    "    for i=1:maxIts\n",
    "        \n",
    "        Tol = norm(g_k0);\n",
    "        if Tol<tolFactor\n",
    "            break;\n",
    "        end\n",
    "        step_k0 = B_k0\\g_k0;\n",
    "        \n",
    "        alpha = 1;\n",
    "        #(f_k1, g_k1, h_k1) = f(x_k0-alpha*step_k0);\n",
    "        (f_k1, g_k1, h_k1) = lagrangian(f, h, x_k0-alpha*step_k0, lambda, rho);\n",
    "        \n",
    "        #Linesearch via Armijo Backtracking\n",
    "        while( f_k1 > f_k0 )\n",
    "            alpha=.5*alpha;\n",
    "            #(f_k1, g_k1, h_k1) = f(x_k0-alpha*step_k0);\n",
    "            (f_k1, g_k1, h_k1) = lagrangian(f, h, x_k0-alpha*step_k0, lambda, rho);\n",
    "            if alpha < ( 1e-2)\n",
    "                break;\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        x_k1 = x_k0 - alpha*(step_k0);\n",
    "        #(f_k1, g_k1, h_k1) = f(x_k1);\n",
    "        (f_k1, g_k1, h_k1) = lagrangian(f, h, x_k1, lambda, rho);\n",
    "        y = g_k1 - g_k0;\n",
    "        s = x_k1 - x_k0;\n",
    "        \n",
    "        #BFGS Update\n",
    "        dB = (y*y')/dot(s,y) - (B_k0*(s*s')*B_k0)/dot(s,B_k0*s);\n",
    "        \n",
    "        B_k0 = B_k0 + dB;\n",
    "        \n",
    "        x_k0 = x_k1;\n",
    "        #(f_k0, g_k0, h_k0) = f(x_k0);\n",
    "        (f_k0, g_k0, h_k0) = lagrangian(f, h, x_k0, lambda, rho);\n",
    "        its+=1;\n",
    " \n",
    "    end\n",
    "    x = x_k0;\n",
    "    f = f_k0;\n",
    "    B = B_k0;\n",
    "    \n",
    " \n",
    "    #return(f,its,Tol,B,x);\n",
    "    return (x);\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "augLagrange (generic function with 1 method)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function augLagrange(f, h, x_k0, maxIts, optTol)\n",
    "    \n",
    "    #implementation of the augmented lagrangian method\n",
    "    lambda_k0 = -.55;\n",
    "    rho_k0 = .001;\n",
    "    beta = 1.5;\n",
    "    gamma = .05;\n",
    "    \n",
    "    its = 1;\n",
    "    \n",
    "    #initial call to objective and constraint functions\n",
    "    (f_k0, fgrad_k0, fhes_k0) = f(x_k0);\n",
    "    (h_k0, hgrad_k0, hhes_k0) = h(x_k0);\n",
    "   \n",
    "    tol = norm(fgrad_k0 - lambda_k0*hgrad_k0)*optTol;\n",
    "    \n",
    "    for i=1:maxIts\n",
    "        \n",
    "        #check the tolertance condition\n",
    "        Tol_H = norm(h_k0);\n",
    "        Tol_L = norm(fgrad_k0 - lambda_k0*hgrad_k0);\n",
    "        if (Tol_L < tol) && (Tol_H < tol)\n",
    "            break;\n",
    "        end\n",
    "        \n",
    "        #find the minimizer for the lambda and rho pair\n",
    "        x_k1 = newtmin_BFGS_HW5(f, h, x_k0, lambda_k0, rho_k0, 200, 1e-6); \n",
    "        (f_k1, fgrad_k1, fhes_k1) = f(x_k1);\n",
    "        (h_k1, hgrad_k1, hhes_k1) = h(x_k1);\n",
    "        \n",
    "        if norm(h_k1) <= gamma*norm(h_k0);\n",
    "            rho_k1 = rho_k0;\n",
    "            lambda_k1 = lambda_k0 + rho_k0*h_k1;\n",
    "        else rho_k1 = beta*rho_k0;\n",
    "            lambda_k1 = lambda_k0;\n",
    "        end  \n",
    "        #lambda_k1 = lambda_k0 + rho_k1*h_k1;\n",
    "        lambda_k0 = lambda_k1;\n",
    "        rho_k0 = rho_k1;\n",
    "        x_k0 = x_k1;\n",
    "        f_k = f_k1;\n",
    "        h_k = h_k1;\n",
    "        its+=1;\n",
    "    end\n",
    "    \n",
    "    lambda = lambda_k0;\n",
    "    rho = rho_k0;\n",
    "    x = x_k0;\n",
    "    f_k = f_k0;\n",
    "    h_k = h_k0;\n",
    "    Tol = norm(fgrad_k0 - lambda_k0*hgrad_k0);\n",
    "    \n",
    "    return (x,rho,lambda,f_k,h_k,its,Tol);\n",
    "end\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(68682.57819999999,[-114179.95599999998,-114179.95599999998],\n",
       "2x2 Array{Float64,2}:\n",
       " 1.0  0.0\n",
       " 0.0  1.0)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(L_k0, Lgrd_k0, Lhes_k0) = lagrangian(x -> (F_1(x), F_1_grd(x), F_1_hes(x)),x -> (H_1(x), H_1_grd(x), H_1_hes(x)), [-1.9,-1.9], 3731, 6471)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([NaN,NaN],1.652919910788207e32,-0.55,1.0,0.0,201,5.852349955359813)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x,rho,lambda,f_k,h_k,its,Tol)=augLagrange(x -> (F_6(x), F_6_grd(x), F_6_hes(x)),x -> (H_6(x), H_6_grd(x), H_6_hes(x)), F_6_x0, 200, .01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.0",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
